fraud_autoencoder/
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ main.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ ‚îî‚îÄ‚îÄ raw/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ ‚îú‚îÄ‚îÄ ingest_data.py
‚îÇ ‚îú‚îÄ‚îÄ data_cleaning.py
‚îÇ ‚îú‚îÄ‚îÄ feature_engineering.py
‚îÇ ‚îú‚îÄ‚îÄ model.py
‚îÇ ‚îú‚îÄ‚îÄ train.py
‚îÇ ‚îú‚îÄ‚îÄ evaluate.py
‚îÇ ‚îî‚îÄ‚îÄ config.py
‚îÇ
‚îî‚îÄ‚îÄ kaggle.json (added at runtime or via environment)

Dockerfile 
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "main.py"]

Requirements 
pandas
numpy
scikit-learn
matplotlib
seaborn
torch
kaggle

src/config.py
import os

DATA_DIR = "data/raw"
UNZIP_DIR = "ieee_cis"
KAGGLE_COMPETITION = "ieee-fraud-detection"

src/ingest_data.py
import os
import json
import subprocess
from src.config import DATA_DIR, UNZIP_DIR, KAGGLE_COMPETITION

def download_data_if_needed():
if os.path.exists(os.path.join(DATA_DIR, "train_transaction.csv")):
print("‚úÖ Data already exists, skipping download.")
return

print("üì¶ Downloading data from Kaggle...")
os.makedirs("/root/.kaggle", exist_ok=True)
with open("/root/.kaggle/kaggle.json", "w") as f:
json.dump({
"username": os.environ["KAGGLE_USERNAME"],
"key": os.environ["KAGGLE_KEY"]
}, f)
os.chmod("/root/.kaggle/kaggle.json", 0o600)

subprocess.run([
"kaggle", "competitions", "download", "-c", KAGGLE_COMPETITION
], check=True)
subprocess.run([
"unzip", "-q", "ieee-fraud-detection.zip", "-d", UNZIP_DIR
], check=True)
os.makedirs(DATA_DIR, exist_ok=True)
os.rename(f"{UNZIP_DIR}/train_transaction.csv", f"{DATA_DIR}/train_transaction.csv")
os.rename(f"{UNZIP_DIR}/train_identity.csv", f"{DATA_DIR}/train_identity.csv")
print("‚úÖ Data ingested and saved to disk.")

src/data_cleaning
import pandas as pd
import os
from src.config import DATA_DIR

def load_and_clean_data():
print("üßº Loading and cleaning data...")
trans = pd.read_csv(os.path.join(DATA_DIR, "train_transaction.csv"))
iden = pd.read_csv(os.path.join(DATA_DIR, "train_identity.csv"))

df = trans.merge(iden, on="TransactionID", how="left")

drop_cols = [
"TransactionID", "TransactionDT", "ProductCD", "DeviceType", "DeviceInfo",
"id_30", "id_31", "id_33", "id_34", "id_35", "id_36", "id_37", "id_38"
]
df = df.drop(columns=drop_cols, errors='ignore')
df = df.dropna(thresh=len(df) * 0.5, axis=1)
df = df.fillna(0)

from sklearn.preprocessing import LabelEncoder
for col in df.select_dtypes(include="object").columns:
df[col] = LabelEncoder().fit_transform(df[col].astype(str))

print(f"‚úÖ Final shape: {df.shape}")
return df

src/feature_engineering.py
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

def preprocess_data(df):
print("üß™ Preprocessing and splitting data...")
X = df.drop(columns=["isFraud"])
y = df["isFraud"]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
X_scaled, y, test_size=0.2, stratify=y, random_state=42
)

X_train_ae = X_train[y_train == 0]
return X_train_ae, X_test, y_train, y_test

src/model.py
import torch.nn as nn

class Autoencoder(nn.Module):
def __init__(self, input_dim):
super().__init__()
self.encoder = nn.Sequential(
nn.Linear(input_dim, 64),
nn.ReLU(),
nn.Linear(64, 32)
)
self.decoder = nn.Sequential(
nn.Linear(32, 64),
nn.ReLU(),
nn.Linear(64, input_dim)
)

def forward(self, x):
return self.decoder(self.encoder(x))

src/train.py
import torch
from torch.utils.data import DataLoader, TensorDataset
from src.model import Autoencoder

def train_autoencoder(X_train_ae, input_dim, epochs=30):
print("üèãÔ∏è Training autoencoder...")
model = Autoencoder(input_dim)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = torch.nn.MSELoss()

dataset = TensorDataset(torch.tensor(X_train_ae, dtype=torch.float32))
loader = DataLoader(dataset, batch_size=256, shuffle=True)

model.train()
for epoch in range(epochs):
total_loss = 0
for batch in loader:
x = batch[0]
optimizer.zero_grad()
out = model(x)
loss = criterion(out, x)
loss.backward()
optimizer.step()
total_loss += loss.item()
print(f"üìâ Epoch {epoch+1} - Loss: {total_loss:.2f}")
return model

src/evaluate.py
import numpy as np
import torch
from sklearn.metrics import classification_report, roc_auc_score

def evaluate_autoencoder(model, X_train_ae, X_test, y_test, percentile=90):
print("üîé Detecting anomalies...")
model.eval()
with torch.no_grad():
recon_test = model(torch.tensor(X_test, dtype=torch.float32)).numpy()
recon_train = model(torch.tensor(X_train_ae, dtype=torch.float32)).numpy()

test_errors = ((X_test - recon_test) ** 2).mean(axis=1)
threshold = np.percentile(((X_train_ae - recon_train) ** 2).mean(axis=1), percentile)

print(f"üìè Threshold ({percentile}th percentile): {threshold:.4f}")
y_pred = (test_errors > threshold).astype(int)

print("\nüìÑ Classification Report:")
print(classification_report(y_test, y_pred))
print(f"ROC-AUC Score: {roc_auc_score(y_test, test_errors):.4f}")

main.py

from src import ingest_data, data_cleaning, feature_engineering, train, evaluate

if __name__ == "__main__":
ingest_data.download_data_if_needed()
df = data_cleaning.load_and_clean_data()
X_train_ae, X_test, y_train, y_test = feature_engineering.preprocess_data(df)
model = train.train_autoencoder(X_train_ae, input_dim=X_train_ae.shape[1], epochs=30)
evaluate.evaluate_autoencoder(model, X_train_ae, X_test, y_test, percentile=90)

üê≥ Deployment on DigitalOcean
Create a Droplet (Ubuntu)

SSH in and clone repo

Add your kaggle.json with environment variables


Bash
export KAGGLE_USERNAME=tomwood6666
export KAGGLE_KEY=4ae9279d450f7ee03553f135b7457033

Build and run
docker build -t fraud-ae .
docker run -e KAGGLE_USERNAME=$KAGGLE_USERNAME -e KAGGLE_KEY=$KAGGLE_KEY fraud-ae
