For a solid, production-grade experiment tracking workflow using a feature factory with multiple feature sets, the best approach is modular, reproducible, and extensible. Hereâ€™s how pros typically do it:

âœ… TL;DR Best Practice Summary
Use CLI arguments to select feature sets (--config basic, --config behavioural)

Have separate config YAML/JSON files for each feature set

Baseline features should be included in all configs as shared logic, not conditionally stacked

Use a unified run_pipeline.py script that takes the config as input and logs to MLflow/W&B

Log feature set name, hyperparameters, model version, and metrics in experiment tracking

Keep feature engineering modular using a factory pattern or strategy injection

Avoid boolean flags like USE_BEHAVIOURAL=True inside a single config â€” thatâ€™s brittle and doesnâ€™t scale for more than 2 variants.

ğŸ”§ Project Structure Recommendation
project/
â”œâ”€â”€ run_pipeline.py
â”œâ”€â”€ configs/
â”‚ â”œâ”€â”€ baseline.yaml
â”‚ â”œâ”€â”€ behavioural.yaml
â”‚ â””â”€â”€ temporal.yaml
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ ingest_data.py
â”‚ â”œâ”€â”€ data_cleaning.py
â”‚ â”œâ”€â”€ feature_factory/
â”‚ â”‚ â”œâ”€â”€ __init__.py
â”‚ â”‚ â”œâ”€â”€ baseline.py
â”‚ â”‚ â”œâ”€â”€ behavioural.py
â”‚ â”‚ â””â”€â”€ temporal.py
â”‚ â””â”€â”€ model_training.py
â””â”€â”€ logs/

How to Structure the CLI


Your run_pipeline.py might look like:

python run_pipeline.py --config baseline
python run_pipeline.py --config behavioural

Or with full path:

python run_pipeline.py --config configs/baseline.yaml

Feature Factory Strategy Pattern (recommended)


You define feature logic like this:

class FeatureEngineer(ABC):
@abstractmethod
def generate_features(self, df: pd.DataFrame) -> pd.DataFrame:
pass

class BaselineFeatures(FeatureEngineer):
def generate_features(self, df):
# Add common features
return df

class BehaviouralFeatures(FeatureEngineer):
def generate_features(self, df):
df = BaselineFeatures().generate_features(df)
# Add behavioural rolling features
return df

Then in your pipeline:

feature_strategy = FeatureFactory.create(config["feature_strategy"])
df = feature_strategy.generate_features(df)


Experiment Tracking with MLflow/W&B


Every run should log:

Feature strategy name (e.g. baseline, behavioural)

Model type and version

Metrics (AUC, precision, etc.)

Dataset hash (optional)

Timestamp

Git commit hash (ideal for reproducibility)


Example:
import mlflow

mlflow.start_run(run_name=config["feature_strategy"])
mlflow.log_params(config)
mlflow.log_metrics(metrics)
mlflow.end_run()

What to Avoid
âŒ Hardcoding feature toggles in one config file

âŒ Running baseline then stacking behavioural features in one run â€” this creates coupling and bad tracking

âŒ Boolean flags like USE_BEHAVIOURAL=True â€” doesnâ€™t scale to 3+ strategies

âœ… Final Recommendations

Goal

Recommendation

Compare feature strategies

One config file per strategy

Reproducible runs

CLI + config loading + MLflow

Feature engineering

Strategy pattern with clean inheritance

Scaling experiments

Use loops/schedulers to automate config sweeps

Deployment

Lock best config into production.yaml