import os
import sys
# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend for Docker
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, precision_recall_curve, precision_score, recall_score, f1_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import json
import joblib
import tensorflow as tf
import yaml
import logging
from pathlib import Path

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

STATIC_DIR = "static"
SCALER_PATH = "models/final_model_scaler.pkl"
MODEL_PATH = "models/final_model.h5"
MODEL_INFO_PATH = "models/final_model_info.yaml"
DATA_PATH = "data/cleaned/ecommerce_cleaned.csv"
INTERMEDIATE_DIR = "intermediate"
PREDICTIONS_DIR = "predictions"

def ensure_directories():
    """Ensure all required directories exist"""
    for directory in [STATIC_DIR, INTERMEDIATE_DIR, PREDICTIONS_DIR]:
        os.makedirs(directory, exist_ok=True)

def load_intermediate_files():
    """Load files generated by run_pipeline.py"""
    files = {}
    
    # Load anomaly scores
    anomaly_scores_path = os.path.join(INTERMEDIATE_DIR, "anomaly_scores.csv")
    if os.path.exists(anomaly_scores_path):
        files['anomaly_scores'] = pd.read_csv(anomaly_scores_path)
        logger.info(f"Loaded anomaly scores: {len(files['anomaly_scores'])} records")
    
    # Load latent space
    latent_space_path = os.path.join(INTERMEDIATE_DIR, "latent_space.npy")
    if os.path.exists(latent_space_path):
        files['latent_space'] = np.load(latent_space_path)
        logger.info(f"Loaded latent space: {files['latent_space'].shape}")
    
    # Load threshold info
    threshold_path = os.path.join(INTERMEDIATE_DIR, "threshold_info.yaml")
    if os.path.exists(threshold_path):
        with open(threshold_path, 'r') as f:
            files['threshold_info'] = yaml.safe_load(f)
        logger.info(f"Loaded threshold info: {files['threshold_info']}")
    
    # Load predictions
    predictions_path = os.path.join(PREDICTIONS_DIR, "labelled_predictions.csv")
    if os.path.exists(predictions_path):
        files['predictions'] = pd.read_csv(predictions_path)
        logger.info(f"Loaded predictions: {len(files['predictions'])} records")
    
    return files

def save_figure(fig, filename, dpi=300):
    """Save figure with error handling"""
    try:
        path = os.path.join(STATIC_DIR, filename)
        fig.savefig(path, dpi=dpi, bbox_inches='tight')
        plt.close(fig)
        logger.info(f"Saved: {filename}")
        return True
    except Exception as e:
        logger.error(f"Error saving {filename}: {e}")
        plt.close(fig)
        return False

def generate_graph_1_class_distribution(df):
    """Graph 1: Class Distribution - Show the severe imbalance"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Bar chart
    class_counts = df['is_fraudulent'].value_counts()
    colors = ['#2E8B57', '#DC143C']
    bars = ax1.bar(['Legitimate', 'Fraudulent'], class_counts.values, color=colors, alpha=0.8)
    ax1.set_title('Transaction Class Distribution', fontsize=16, fontweight='bold')
    ax1.set_ylabel('Number of Transactions', fontsize=12)
    
    # Add value labels on bars
    for bar, count in zip(bars, class_counts.values):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                f'{count:,}', ha='center', va='bottom', fontweight='bold')
    
    # Pie chart
    labels = ['Legitimate', 'Fraudulent']
    sizes = class_counts.values
    explode = (0, 0.1)
    ax2.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',
            startangle=90, shadow=True)
    ax2.set_title('Fraud Rate', fontsize=16, fontweight='bold')
    
    plt.tight_layout()
    save_figure(fig, '01_class_distribution.png')

def generate_graph_2_reconstruction_error_histogram(anomaly_scores_df):
    """Graph 2: Reconstruction Error Histogram - Shows natural separation"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Overall histogram
    ax1.hist(anomaly_scores_df['anomaly_score'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.axvline(anomaly_scores_df['anomaly_score'].quantile(0.95), color='red', linestyle='--', 
                linewidth=2, label='95th Percentile Threshold')
    ax1.set_title('Reconstruction Error Distribution', fontsize=16, fontweight='bold')
    ax1.set_xlabel('Reconstruction Error (MSE)', fontsize=12)
    ax1.set_ylabel('Frequency', fontsize=12)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Log scale for better visualization
    ax2.hist(anomaly_scores_df['anomaly_score'], bins=50, alpha=0.7, color='lightcoral', edgecolor='black')
    ax2.axvline(anomaly_scores_df['anomaly_score'].quantile(0.95), color='red', linestyle='--', 
                linewidth=2, label='95th Percentile Threshold')
    ax2.set_yscale('log')
    ax2.set_title('Reconstruction Error Distribution (Log Scale)', fontsize=16, fontweight='bold')
    ax2.set_xlabel('Reconstruction Error (MSE)', fontsize=12)
    ax2.set_ylabel('Frequency (Log Scale)', fontsize=12)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_figure(fig, '02_reconstruction_error_histogram.png')

def generate_graph_3_autoencoder_architecture(model):
    """Graph 3: Autoencoder Architecture Diagram"""
    try:
        from tensorflow.keras.utils import plot_model
        plot_path = os.path.join(STATIC_DIR, '03_autoencoder_architecture.png')
        plot_model(model, to_file=plot_path, show_shapes=True, show_layer_names=True, 
                  dpi=120, rankdir='TB')
        logger.info("Generated autoencoder architecture diagram")
    except Exception as e:
        logger.error(f"Could not generate architecture diagram: {e}")

def generate_graph_4_training_history(model_info):
    """Graph 4: Training vs Validation Loss Curve"""
    if 'training_history' in model_info:
        history = model_info['training_history']
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Loss plot
        ax1.plot(history['loss'], label='Training Loss', color='blue', linewidth=2)
        if 'val_loss' in history:
            ax1.plot(history['val_loss'], label='Validation Loss', color='red', linewidth=2)
        ax1.set_title('Training History - Loss', fontsize=16, fontweight='bold')
        ax1.set_xlabel('Epoch', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Reconstruction error plot
        if 'reconstruction_error' in history:
            ax2.plot(history['reconstruction_error'], label='Reconstruction Error', color='green', linewidth=2)
            ax2.set_title('Training History - Reconstruction Error', fontsize=16, fontweight='bold')
            ax2.set_xlabel('Epoch', fontsize=12)
            ax2.set_ylabel('Reconstruction Error', fontsize=12)
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        save_figure(fig, '04_training_history.png')
    else:
        logger.warning("No training history available in model info")

def generate_graph_5_roc_curve(y_true, anomaly_scores):
    """Graph 5: AUC-ROC Curve"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ROC Curve
    fpr, tpr, _ = roc_curve(y_true, anomaly_scores)
    roc_auc = auc(fpr, tpr)
    
    ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
    ax1.set_xlim([0.0, 1.0])
    ax1.set_ylim([0.0, 1.05])
    ax1.set_xlabel('False Positive Rate', fontsize=12)
    ax1.set_ylabel('True Positive Rate', fontsize=12)
    ax1.set_title('ROC Curve', fontsize=16, fontweight='bold')
    ax1.legend(loc="lower right")
    ax1.grid(True, alpha=0.3)
    
    # AUC Score breakdown
    auc_breakdown = {
        'Excellent': roc_auc >= 0.9,
        'Good': 0.8 <= roc_auc < 0.9,
        'Fair': 0.7 <= roc_auc < 0.8,
        'Poor': roc_auc < 0.7
    }
    
    colors = ['#2E8B57', '#32CD32', '#FFD700', '#DC143C']
    labels = list(auc_breakdown.keys())
    values = [1 if auc_breakdown[label] else 0 for label in labels]
    
    bars = ax2.bar(labels, values, color=colors, alpha=0.8)
    ax2.set_title(f'Model Performance: AUC = {roc_auc:.3f}', fontsize=16, fontweight='bold')
    ax2.set_ylabel('Performance Level', fontsize=12)
    ax2.set_ylim(0, 1.2)
    
    # Highlight the current performance level
    for i, (label, value) in enumerate(zip(labels, values)):
        if value == 1:
            ax2.text(i, value + 0.05, 'Current', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    save_figure(fig, '05_roc_curve.png')

def generate_graph_6_precision_recall_curve(y_true, anomaly_scores):
    """Graph 6: Precision-Recall Curve"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Precision-Recall Curve
    precision, recall, _ = precision_recall_curve(y_true, anomaly_scores)
    pr_auc = auc(recall, precision)
    
    ax1.plot(recall, precision, color='purple', lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')
    ax1.set_xlim([0.0, 1.0])
    ax1.set_ylim([0.0, 1.05])
    ax1.set_xlabel('Recall', fontsize=12)
    ax1.set_ylabel('Precision', fontsize=12)
    ax1.set_title('Precision-Recall Curve', fontsize=16, fontweight='bold')
    ax1.legend(loc="lower left")
    ax1.grid(True, alpha=0.3)
    
    # Precision vs Recall trade-off
    thresholds = np.linspace(0, 1, 100)
    precisions = []
    recalls = []
    
    for threshold in thresholds:
        predictions = (anomaly_scores > threshold).astype(int)
        if np.sum(predictions) > 0:
            precisions.append(precision_score(y_true, predictions, zero_division=0))
            recalls.append(recall_score(y_true, predictions, zero_division=0))
        else:
            precisions.append(1.0)
            recalls.append(0.0)
    
    ax2.plot(thresholds, precisions, label='Precision', color='blue', linewidth=2)
    ax2.plot(thresholds, recalls, label='Recall', color='red', linewidth=2)
    ax2.set_xlabel('Threshold', fontsize=12)
    ax2.set_ylabel('Score', fontsize=12)
    ax2.set_title('Precision vs Recall Trade-off', fontsize=16, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_figure(fig, '06_precision_recall_curve.png')

def generate_graph_7_threshold_sweep(y_true, anomaly_scores):
    """Graph 7: Threshold Sweep Curve"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Threshold sweep
    percentiles = np.arange(90, 100, 0.5)
    thresholds = np.percentile(anomaly_scores, percentiles)
    
    f1_scores = []
    precision_scores = []
    recall_scores = []
    
    for threshold in thresholds:
        predictions = (anomaly_scores > threshold).astype(int)
        if np.sum(predictions) > 0:
            precision = precision_score(y_true, predictions, zero_division=0)
            recall = recall_score(y_true, predictions, zero_division=0)
            f1 = f1_score(y_true, predictions, zero_division=0)
        else:
            precision = 1.0
            recall = 0.0
            f1 = 0.0
        
        precision_scores.append(precision)
        recall_scores.append(recall)
        f1_scores.append(f1)
    
    # Plot metrics vs percentile
    ax1.plot(percentiles, f1_scores, label='F1 Score', color='green', linewidth=2)
    ax1.plot(percentiles, precision_scores, label='Precision', color='blue', linewidth=2)
    ax1.plot(percentiles, recall_scores, label='Recall', color='red', linewidth=2)
    ax1.axvline(95, color='black', linestyle='--', alpha=0.7, label='Selected (95th)')
    ax1.set_xlabel('Percentile Threshold', fontsize=12)
    ax1.set_ylabel('Score', fontsize=12)
    ax1.set_title('Performance vs Threshold Percentile', fontsize=16, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot metrics vs actual threshold value
    ax2.plot(thresholds, f1_scores, label='F1 Score', color='green', linewidth=2)
    ax2.plot(thresholds, precision_scores, label='Precision', color='blue', linewidth=2)
    ax2.plot(thresholds, recall_scores, label='Recall', color='red', linewidth=2)
    selected_threshold = np.percentile(anomaly_scores, 95)
    ax2.axvline(selected_threshold, color='black', linestyle='--', alpha=0.7, label=f'Selected ({selected_threshold:.3f})')
    ax2.set_xlabel('Threshold Value', fontsize=12)
    ax2.set_ylabel('Score', fontsize=12)
    ax2.set_title('Performance vs Threshold Value', fontsize=16, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_figure(fig, '07_threshold_sweep.png')

def generate_graph_8_confusion_matrix(y_true, anomaly_scores, threshold_percentile=95):
    """Graph 8: Confusion Matrix - Shows model performance at selected threshold"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Calculate threshold and predictions
    threshold = np.percentile(anomaly_scores, threshold_percentile)
    y_pred = (anomaly_scores > threshold).astype(int)
    
    # Create confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    
    # Plot confusion matrix
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
                xticklabels=['Legitimate', 'Fraudulent'],
                yticklabels=['Legitimate', 'Fraudulent'])
    ax1.set_title(f'Confusion Matrix (Threshold: {threshold_percentile}%)', fontsize=16, fontweight='bold')
    ax1.set_xlabel('Predicted', fontsize=12)
    ax1.set_ylabel('Actual', fontsize=12)
    
    # Calculate metrics
    tn, fp, fn, tp = cm.ravel()
    total = tn + fp + fn + tp
    
    accuracy = (tp + tn) / total
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # Create metrics summary
    metrics_text = f"""
    Model Performance Metrics:
    
    Accuracy: {accuracy:.3f}
    Precision: {precision:.3f}
    Recall: {recall:.3f}
    F1-Score: {f1:.3f}
    
    Confusion Matrix Breakdown:
    
    True Negatives: {tn:,} (Legitimate correctly identified)
    False Positives: {fp:,} (Legitimate flagged as fraud)
    False Negatives: {fn:,} (Fraud missed)
    True Positives: {tp:,} (Fraud correctly identified)
    
    Threshold: {threshold:.4f} ({threshold_percentile}th percentile)
    """
    
    ax2.text(0.1, 0.9, metrics_text, transform=ax2.transAxes, fontsize=11, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    ax2.set_xlim(0, 1)
    ax2.set_ylim(0, 1)
    ax2.axis('off')
    ax2.set_title('Performance Summary', fontsize=16, fontweight='bold')
    
    plt.tight_layout()
    save_figure(fig, '08_confusion_matrix.png')

def generate_graph_9_latent_space_visualization(latent_space, y_true):
    """Graph 9: Latent Space Visualization"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # PCA visualization
    if latent_space.shape[1] > 2:
        pca = PCA(n_components=2)
        latent_2d_pca = pca.fit_transform(latent_space)
        
        scatter1 = ax1.scatter(latent_2d_pca[y_true == 0, 0], latent_2d_pca[y_true == 0, 1], 
                              alpha=0.6, label='Legitimate', color='blue', s=20)
        scatter2 = ax1.scatter(latent_2d_pca[y_true == 1, 0], latent_2d_pca[y_true == 1, 1], 
                              alpha=0.8, label='Fraudulent', color='red', s=30)
        ax1.set_xlabel('First Principal Component', fontsize=12)
        ax1.set_ylabel('Second Principal Component', fontsize=12)
        ax1.set_title('Latent Space - PCA Projection', fontsize=16, fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
    
    # t-SNE visualization (if latent space is not too large)
    if latent_space.shape[0] <= 10000:  # t-SNE is slow for large datasets
        try:
            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, latent_space.shape[0]-1))
            latent_2d_tsne = tsne.fit_transform(latent_space)
            
            scatter1 = ax2.scatter(latent_2d_tsne[y_true == 0, 0], latent_2d_tsne[y_true == 0, 1], 
                                  alpha=0.6, label='Legitimate', color='blue', s=20)
            scatter2 = ax2.scatter(latent_2d_tsne[y_true == 1, 0], latent_2d_tsne[y_true == 1, 1], 
                                  alpha=0.8, label='Fraudulent', color='red', s=30)
            ax2.set_xlabel('t-SNE Component 1', fontsize=12)
            ax2.set_ylabel('t-SNE Component 2', fontsize=12)
            ax2.set_title('Latent Space - t-SNE Projection', fontsize=16, fontweight='bold')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        except Exception as e:
            logger.warning(f"t-SNE failed: {e}")
            ax2.text(0.5, 0.5, 't-SNE visualization\nnot available\n(dataset too large)', 
                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)
    else:
        ax2.text(0.5, 0.5, 't-SNE visualization\nnot available\n(dataset too large)', 
                ha='center', va='center', transform=ax2.transAxes, fontsize=12)
    
    plt.tight_layout()
    save_figure(fig, '08_latent_space_visualization.png')

def generate_graph_10_anomaly_scores_over_time(anomaly_scores_df):
    """Graph 10: Anomaly Scores Over Time"""
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
    
    # Check if we have time information
    if 'transaction_hour' in anomaly_scores_df.columns:
        # Anomaly scores by hour
        hourly_scores = anomaly_scores_df.groupby('transaction_hour')['anomaly_score'].mean()
        hourly_counts = anomaly_scores_df.groupby('transaction_hour').size()
        
        ax1.plot(hourly_scores.index, hourly_scores.values, color='red', linewidth=2, marker='o')
        ax1.set_xlabel('Hour of Day', fontsize=12)
        ax1.set_ylabel('Average Anomaly Score', fontsize=12)
        ax1.set_title('Anomaly Scores by Hour of Day', fontsize=16, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # Add transaction volume as background
        ax1_twin = ax1.twinx()
        ax1_twin.bar(hourly_counts.index, hourly_counts.values, alpha=0.3, color='blue', label='Transaction Volume')
        ax1_twin.set_ylabel('Transaction Volume', fontsize=12)
        ax1_twin.legend(loc='upper right')
    else:
        # If no time info, show anomaly scores over transaction index
        ax1.plot(anomaly_scores_df['anomaly_score'].values, color='red', linewidth=1, alpha=0.7)
        ax1.set_xlabel('Transaction Index', fontsize=12)
        ax1.set_ylabel('Anomaly Score', fontsize=12)
        ax1.set_title('Anomaly Scores Over Transactions', fontsize=16, fontweight='bold')
        ax1.grid(True, alpha=0.3)
    
    # Anomaly score distribution over time
    threshold = anomaly_scores_df['anomaly_score'].quantile(0.95)
    high_anomaly = anomaly_scores_df[anomaly_scores_df['anomaly_score'] > threshold]
    
    if 'transaction_hour' in anomaly_scores_df.columns:
        hourly_fraud = high_anomaly.groupby('transaction_hour').size()
        ax2.bar(hourly_fraud.index, hourly_fraud.values, color='red', alpha=0.8)
        ax2.set_xlabel('Hour of Day', fontsize=12)
        ax2.set_ylabel('Number of High Anomaly Transactions', fontsize=12)
        ax2.set_title('High Anomaly Transactions by Hour', fontsize=16, fontweight='bold')
    else:
        # Show distribution of high anomaly scores
        ax2.hist(high_anomaly['anomaly_score'], bins=30, color='red', alpha=0.8, edgecolor='black')
        ax2.set_xlabel('Anomaly Score', fontsize=12)
        ax2.set_ylabel('Frequency', fontsize=12)
        ax2.set_title('Distribution of High Anomaly Scores', fontsize=16, fontweight='bold')
    
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_figure(fig, '10_anomaly_scores_over_time.png')

def generate_graph_11_top_anomalies_table(predictions_df, anomaly_scores_df, n=10):
    """Graph 10: Top-N Anomalies Table/Bar"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))
    
    # Merge predictions with anomaly scores
    if 'anomaly_score' in predictions_df.columns:
        merged_df = predictions_df.copy()
    else:
        merged_df = predictions_df.merge(anomaly_scores_df[['anomaly_score']], 
                                       left_index=True, right_index=True, how='left')
    
    # Get top anomalies
    top_anomalies = merged_df.nlargest(n, 'anomaly_score')
    
    # Bar chart of top anomaly scores
    bars = ax1.barh(range(n), top_anomalies['anomaly_score'].values, color='red', alpha=0.8)
    ax1.set_yticks(range(n))
    ax1.set_yticklabels([f'Transaction {i+1}' for i in range(n)])
    ax1.set_xlabel('Anomaly Score', fontsize=12)
    ax1.set_title(f'Top {n} Anomaly Scores', fontsize=16, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    # Add value labels
    for i, (bar, score) in enumerate(zip(bars, top_anomalies['anomaly_score'].values)):
        ax1.text(score + score*0.01, i, f'{score:.3f}', va='center', fontweight='bold')
    
    # Summary statistics
    stats_text = f"""
    Top {n} Anomalies Summary:
    
    Average Score: {top_anomalies['anomaly_score'].mean():.3f}
    Max Score: {top_anomalies['anomaly_score'].max():.3f}
    Min Score: {top_anomalies['anomaly_score'].min():.3f}
    
    Predicted Fraud: {top_anomalies['predicted_fraud'].sum()}/{n}
    Actual Fraud: {top_anomalies['is_fraudulent'].sum() if 'is_fraudulent' in top_anomalies.columns else 'N/A'}/{n}
    """
    
    ax2.text(0.1, 0.9, stats_text, transform=ax2.transAxes, fontsize=12, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    ax2.set_xlim(0, 1)
    ax2.set_ylim(0, 1)
    ax2.axis('off')
    ax2.set_title('Anomaly Summary', fontsize=16, fontweight='bold')
    
    plt.tight_layout()
    save_figure(fig, '11_top_anomalies.png')

def generate_all_graphs():
    """Generate all 11 graphs following the narrative flow"""
    try:
        logger.info("Starting comprehensive graph generation...")
        ensure_directories()
        
        # Load data and model
        logger.info("Loading data and model...")
        df = pd.read_csv(DATA_PATH)
        model = tf.keras.models.load_model(MODEL_PATH, compile=False)
        
        with open(MODEL_INFO_PATH, 'r') as f:
            model_info = yaml.safe_load(f)
        
        # Load intermediate files
        intermediate_files = load_intermediate_files()
        
        logger.info("Generating narrative flow graphs...")
        
        # Section 1: Problem + Data (use full dataset for class distribution)
        logger.info("1. Class Distribution")
        generate_graph_1_class_distribution(df)
        
        logger.info("2. Reconstruction Error Histogram")
        if 'anomaly_scores' in intermediate_files:
            generate_graph_2_reconstruction_error_histogram(intermediate_files['anomaly_scores'])
        
        # Section 2: Modeling
        logger.info("3. Autoencoder Architecture")
        generate_graph_3_autoencoder_architecture(model)
        
        logger.info("4. Training History")
        generate_graph_4_training_history(model_info)
        
        # Section 3: Evaluation (use test set data only)
        logger.info("5. ROC Curve")
        if 'anomaly_scores' in intermediate_files:
            # Use test set data for evaluation
            test_anomaly_scores = intermediate_files['anomaly_scores']
            y_true = test_anomaly_scores['is_fraudulent'].values
            anomaly_scores = test_anomaly_scores['anomaly_score'].values
            generate_graph_5_roc_curve(y_true, anomaly_scores)
        
        logger.info("6. Precision-Recall Curve")
        if 'anomaly_scores' in intermediate_files:
            generate_graph_6_precision_recall_curve(y_true, anomaly_scores)
        
        logger.info("7. Threshold Sweep")
        if 'anomaly_scores' in intermediate_files:
            generate_graph_7_threshold_sweep(y_true, anomaly_scores)
        
        logger.info("8. Confusion Matrix")
        if 'anomaly_scores' in intermediate_files:
            generate_graph_8_confusion_matrix(y_true, anomaly_scores)
        
        # Section 4: Insight & Action (use test set data)
        logger.info("9. Latent Space Visualization")
        if 'latent_space' in intermediate_files:
            generate_graph_9_latent_space_visualization(intermediate_files['latent_space'], y_true)
        
        logger.info("10. Anomaly Scores Over Time")
        if 'anomaly_scores' in intermediate_files:
            generate_graph_10_anomaly_scores_over_time(intermediate_files['anomaly_scores'])
        
        logger.info("11. Top Anomalies")
        if 'predictions' in intermediate_files:
            generate_graph_11_top_anomalies_table(intermediate_files['predictions'], 
                                                intermediate_files['anomaly_scores'])
        
        logger.info("All graphs generated successfully!")
        return True
        
    except Exception as e:
        logger.error(f"Error in generate_all_graphs: {e}")
        raise

if __name__ == "__main__":
    try:
        generate_all_graphs()
        print("All 11 narrative flow graphs generated successfully!")
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1) 